# EduVision: From Noise to Narrative
[[`arXiv`](www.google.com)][[`pdf`](www.google.com)]
Generating accurate and diverse descriptions of videos using natural language processing is a challenging task due to the complexity of visual information contained in videos. In this research, we present three methods that aim to enhance the precision and diversity of video description models. These methods include curriculum learning by dropout, curriculum learning by noise, and the Mish activation function. Our experimental results demonstrate the effectiveness of our proposed methods in improving the performance of state-of-the-art model.  Additionally, we perform comprehensive ablation studies to analyze the unique contribution of each approach to the final performance of the model. Our research presents a significant contribution to the field of video description, as our proposed methods offer novel solutions to the challenges associated with video description models. By improving the precision and diversity of these models, we believe that our techniques can lead to better understanding and interpretation of video content, benefiting a range of applications, from video search to accessibility for the visually impaired. Our results show that the proposed methods can be applied in various video description models, improving their generalizability and practicality in real-world scenarios. We evaluate our results across four measures to provide a comprehensive analysis of our proposed techniques. We extend the state-of-the-art model by applying curriculum learning and Mish activation function and succeed to improve 4 language metrics (METEOR, ROGUE\_L, CIDEr, Bleu@4) and 2 diversity scores (Div2 and R@4) in 2 datasets (ActivityNet Captions and YouCook2).


## Environment Setup

See the section Environment Setup [VLTinT](https://github.com/UARK-AICV/VLTinT).

## Data Preparation

See the section Data Preparation in [VLTinT](https://github.com/UARK-AICV/VLTinT).

## Training

See the section Training in [VLTinT](https://github.com/UARK-AICV/VLTinT).

## Evaluation

See the section Evaluation in [VLTinT](https://github.com/UARK-AICV/VLTinT).

## Visualization
See the section Visualization in [VLTinT](https://github.com/UARK-AICV/VLTinT).


